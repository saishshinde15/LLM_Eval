{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>\n",
        "\n",
        "# ðŸ›  An LLM Jury as a Custom Metric in Opik\n",
        "\n",
        "An LLM jury consists of multiple independent LLM evaluators that assess an input and aggregate their outputs using ensembling techniques like voting, averaging, or max selection. Compared to a single large model, a jury of smaller, diverse models reduces intra-model bias, achieves better performance, and operates at a much lower cost.\n",
        "\n",
        "For more about LLM juries, read [the original ArXiv paper here](https://arxiv.org/abs/2404.18796)."
      ],
      "metadata": {
        "id": "eO5Qlry4reeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Set up the environment"
      ],
      "metadata": {
        "id": "7mHNKLtivKzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgSfcVsxrWnR"
      },
      "outputs": [],
      "source": [
        "%pip install opik datasets openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll need a [free Opik account](https://www.comet.com/signup?utm_campaign=opik&utm_medium=colab&utm_source=llm_jury_blog) to start running this code (if you already have a Comet account, that works too!). Next, [grab your API key](https://www.comet.com/account-settings/apiKeys?utm_campaign=opik&utm_medium=colab&utm_source=llm_jury_blog) from your `Account Settings` and run the following code:"
      ],
      "metadata": {
        "id": "hCVoimJsvnXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the project name for Opik\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"llm-juries-project\"\n",
        "\n",
        "import opik\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "DWkCIyE9vm4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "# Set OpenAI API key: https://openai.com/\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "BxMXVvyNwLZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenRouter API key: https://openrouter.ai/\n",
        "if \"OPENROUTER_API_KEY\" not in os.environ:\n",
        "  os.environ[\"OPENROUTER_API_KEY\"] = getpass.getpass(\"Enter your OpenRouter API key: \")"
      ],
      "metadata": {
        "id": "EybW6arNwOT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Define the model\n",
        "\n",
        "For more information on `Qwen2.5-3B-Instruct`, see the [Hugging Face model card here](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct).\n",
        "\n",
        "We'll be using `Qwen2.5-3B-Instruct` to generate answers to questions in the Natural Questions (NQ) dataset from Google Research ([see below](https://colab.research.google.com/drive/1Lt-4rvNIYPhgCMpaTd2N6GxJu9LkfcE5#scrollTo=XCsp2QnMvNyb))."
      ],
      "metadata": {
        "id": "fDdcZ2M2vN7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "hU0vakZ-vi71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Define custom functions\n",
        "\n",
        "By adding tracking to our LLM application, we'll have full visibility into each evaluation run. In the example below we use the `@track` decorator, but there are other ways of adding tracking to your code outlined in the [Opik documentation](https://www.comet.com/docs/opik/tracing/log_traces?utm_campaign=opik&utm_medium=colab&utm_source=LLM_Jury_blog).\n",
        "\n",
        "Here we define a function to generate responses to the input questions from the dataset we'll define in the next few steps."
      ],
      "metadata": {
        "id": "gGIfzCQzvN4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik import track\n",
        "\n",
        "@track\n",
        "def generate_answer(input_question: str) -> str:\n",
        "  \"\"\"Generates an answer based on the input question using the loaded LLM.\"\"\"\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": input_question}\n",
        "  ]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        "  )\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        "  )\n",
        "  generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "  ]\n",
        "\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  return response"
      ],
      "metadata": {
        "id": "AxLw861gwoKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@track\n",
        "def evaluation_task(data):\n",
        "    \"\"\"Evaluates the LLM output given a dataset sample.\"\"\"\n",
        "    llm_output = generate_answer(data['question'])\n",
        "    return {\"output\": llm_output}"
      ],
      "metadata": {
        "id": "xrqH1qntwuw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Define our metric\n",
        "\n",
        "Opik has several [built-in evaluation metrics](https://www.comet.com/docs/opik/evaluation/metrics/overview), but also supports [custom metric definitions](https://www.comet.com/docs/opik/evaluation/metrics/custom_metric) using [Opik's BaseMetric class](https://github.com/comet-ml/opik/blob/main/sdks/python/src/opik/evaluation/metrics/base_metric.py). Here, we build a custom metric that calls each of the three models and aggregates their scores.\n",
        "\n",
        "For our particular use case, we want the models to return structured outputs in the form of valid JSON objects. For this, we'll define the exact structure we're looking for in variable called `response_format`.\n",
        "\n",
        "For more information on what a JSON schema is, [see here](https://json-schema.org/overview/what-is-jsonschema). For more information on how to use JSON schemas as structured outputs with OpenAI, [see here](https://platform.openai.com/docs/guides/structured-outputs)."
      ],
      "metadata": {
        "id": "AxMkl4eQvN1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON schema for hallucination scoring response_format\n",
        "RESPONSE_FORMAT = {\n",
        "      \"type\": \"json_schema\",\n",
        "      \"json_schema\": {\n",
        "        \"name\": \"hallucination_score\",\n",
        "        \"strict\": True,\n",
        "        \"schema\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"score\": {\n",
        "              \"type\": \"number\",\n",
        "              \"description\": \"A hallucination score between 0 and 1\"\n",
        "            },\n",
        "            \"reason\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The reasoning for the assessed hallucination score\"\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"score\", \"reason\"],\n",
        "          \"additionalProperties\": False\n",
        "        }\n",
        "      }\n",
        "    }"
      ],
      "metadata": {
        "id": "Xre71Z3YxK-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define our LLM Jury metric"
      ],
      "metadata": {
        "id": "j0_MAlC8-r5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import base_metric, score_result\n",
        "from opik.evaluation import models\n",
        "import json\n",
        "from typing import Any\n",
        "from openai import OpenAI\n",
        "from opik.integrations.openai import track_openai\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LLMJuryMetric(base_metric.BaseMetric):\n",
        "  \"\"\"Metric to evaluate LLM outputs for factual accuracy using multiple models and an avergae voting function.\"\"\"\n",
        "    def __init__(self, name: str = \"LLM Jury\"):\n",
        "        self.name = name\n",
        "        self.llm_client = track_openai(OpenAI(base_url=\"https://openrouter.ai/api/v1\",\n",
        "                                              api_key=os.getenv(\"OPENROUTER_API_KEY\"),)\n",
        "        )\n",
        "        self.prompt_template = \"\"\"\n",
        "        You are an impartial judge evaluating the following claim for factual accuracy. Analyze it carefully\n",
        "        and respond with a number between 0 and 1: 1 if completely accurate, 0.5 if mixed accuracy, or 0 if inaccurate.\n",
        "        Then provide one brief sentence explaining your ruling.\n",
        "\n",
        "        The format of the your response should be a JSON object with no additional text or backticks that follows the format:\n",
        "        {{\n",
        "            \"score\": <score between 0 and 1>,\n",
        "            \"reason\": \"<reason for the score>\"\n",
        "        }}\n",
        "\n",
        "        Claim to evaluate: {output}\n",
        "\n",
        "        Response:\n",
        "        \"\"\"\n",
        "        self.model_names = [\"openai/gpt-4o-mini\", \"mistralai/mistral-small-24b-instruct-2501\", \"cohere/command-r-08-2024\"]\n",
        "    def score(self, output: str, **ignored_kwargs: Any):\n",
        "        \"\"\"\n",
        "        Score the output of an LLM.\n",
        "\n",
        "        Args:\n",
        "            output: The output of an LLM to score.\n",
        "            **ignored_kwargs: Any additional keyword arguments. This is important so that the metric can be used in the `evaluate` function.\n",
        "        \"\"\"\n",
        "\n",
        "        # Construct the prompt based on the output of the LLM\n",
        "        prompt = self.prompt_template.format(output=output)\n",
        "\n",
        "        completions = []\n",
        "\n",
        "        for model in self.model_names:\n",
        "          try:\n",
        "              completion = self.llm_client.chat.completions.create(\n",
        "                  model=model,\n",
        "                  messages=[\n",
        "                      {\n",
        "                          \"role\": \"user\",\n",
        "                          \"content\": prompt\n",
        "                          }\n",
        "                      ],\n",
        "                  response_format=RESPONSE_FORMAT\n",
        "                  )\n",
        "\n",
        "              response_data = json.loads(completion.choices[0].message.content)\n",
        "              completions.append(response_data)\n",
        "          except (json.JSONDecodeError, AttributeError, IndexError):\n",
        "              print(f\"Error parsing response from model {model}: {completion}\")\n",
        "              continue  # Skip this model if an error occurs\n",
        "\n",
        "        if completions:\n",
        "              avg_score = np.mean([float(response[\"score\"]) for response in completions])\n",
        "              reasons = {self.model_names[i]: response[\"reason\"] for i, response in enumerate(completions)}\n",
        "\n",
        "        else:\n",
        "              avg_score = 0.0\n",
        "              reasons = \"No valid responses received.\"\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            name=self.name,\n",
        "            value=avg_score,\n",
        "            reason=str(reasons)\n",
        "        )"
      ],
      "metadata": {
        "id": "65691j81xPUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Create the Opik Dataset\n",
        "\n",
        "For this experiment, we'll be using the articles contained in the [Natural Questions (NQ) dataset, created by Google Reseach and hosted by Hugging Face](https://huggingface.co/datasets/google-research-datasets/nq_open?library=datasets)."
      ],
      "metadata": {
        "id": "XCsp2QnMvNyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(\"google-research-datasets/nq_open\")['train']"
      ],
      "metadata": {
        "id": "1E71d0gEw79i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess dataset\n",
        "# Take only first 100 rows\n",
        "df = ds.to_pandas().iloc[:100,:]\n",
        "# Convert any list items to arrays\n",
        "df = df.map(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
        "# Rename column to align with variables in our custom functions above\n",
        "df.rename(columns={\"answer\":\"reference\"}, inplace=True)"
      ],
      "metadata": {
        "id": "65GIslpWw8pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opik import Opik\n",
        "\n",
        "# Log dataset to Opik\n",
        "client = Opik()\n",
        "dataset = client.get_or_create_dataset(name=\"NQ-subset\")\n",
        "dataset.insert_from_pandas(df)"
      ],
      "metadata": {
        "id": "AdwJwgKxw4_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ Evaluate"
      ],
      "metadata": {
        "id": "4COLYe4_vZh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate our custom LLM Jury metric\n",
        "LLMJuryMetric = LLMJuryMetric()"
      ],
      "metadata": {
        "id": "a6FbGjUaxSwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation import evaluate\n",
        "\n",
        "# Perform the evaluation\n",
        "evaluation = evaluate(\n",
        "    experiment_name=\"My LLM Jury Experiment\",\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=[LLMJuryMetric],\n",
        "    task_threads=1\n",
        ")"
      ],
      "metadata": {
        "id": "tu0omoC4xUAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have any additional questions?\n",
        "- Check out [Opik's full documentation here](https://www.comet.com/docs/opik/)\n",
        "- [Connect with us on Slack!](chat.comet.com)"
      ],
      "metadata": {
        "id": "1JGl6U8ICb4n"
      }
    }
  ]
}